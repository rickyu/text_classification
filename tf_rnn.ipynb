{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import sklearn\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/ad1/ch_ad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>国产来喽 二嫂最好https://pan.baidu.com/mbox/homepage?s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>本人因个人原因要转出一套滑板（BAKER板面 一副国产桥和轮子）。买的时间不长 也没有太燥 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>这是一部国产版的杨永信+豫章学院+校园暴力的动漫。果然学校是另外一个社会。 你们是否也经历过...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>亲爱的，新号三天不能发私信，加V心给你吧：15968296083 我给你食谱，一定坚持，加油...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>我给你食谱，如果能坚持下来一个月瘦10斤，20斤没问题的，实在不行，我也有另外的方法，想要就...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>加V心：snow0548，有这个哟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>充电打伙计28包邮 喜欢加唯心N07543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>欢乐抓娃娃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>有没有玩欢乐抓娃娃的呀，输入我的邀请码可以得币呦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>欢乐抓娃娃，邀请码963280得30个币，抓到娃娃免费到家</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label                                               text\n",
       "0           0      1  国产来喽 二嫂最好https://pan.baidu.com/mbox/homepage?s...\n",
       "1           1      1  本人因个人原因要转出一套滑板（BAKER板面 一副国产桥和轮子）。买的时间不长 也没有太燥 ...\n",
       "2           2      1  这是一部国产版的杨永信+豫章学院+校园暴力的动漫。果然学校是另外一个社会。 你们是否也经历过...\n",
       "3           3      1  亲爱的，新号三天不能发私信，加V心给你吧：15968296083 我给你食谱，一定坚持，加油...\n",
       "4           4      1  我给你食谱，如果能坚持下来一个月瘦10斤，20斤没问题的，实在不行，我也有另外的方法，想要就...\n",
       "5           5      1                                  加V心：snow0548，有这个哟\n",
       "6           6      1                              充电打伙计28包邮 喜欢加唯心N07543\n",
       "7           7      1                                              欢乐抓娃娃\n",
       "8           8      1                           有没有玩欢乐抓娃娃的呀，输入我的邀请码可以得币呦\n",
       "9           9      1                      欢乐抓娃娃，邀请码963280得30个币，抓到娃娃免费到家"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = df_train['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = list(class_list.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = sklearn.preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.fit(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = keras.utils.to_categorical(class_list.map(lambda x: label_encoder.transform([x])), num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(texts):\n",
    "    return texts.map(lambda x:' '.join(list(jieba.cut(x))))\n",
    "texts = preprocess_text(data_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    国产 来 喽   二嫂 最好 https : / / pan . baidu . com /...\n",
       "1    本人 因 个人 原因 要 转出 一套 滑板 （ BAKER 板面   一副 国产 桥 和 轮...\n",
       "2    这是 一部 国产 版 的 杨永信 + 豫章 学院 + 校园 暴力 的 动漫 。 果然 学校 ...\n",
       "3    亲爱 的 ， 新 号 三天 不能 发 私信 ， 加 V 心给 你 吧 ： 159682960...\n",
       "4    我 给 你 食谱 ， 如果 能 坚持 下来 一个月 瘦 10 斤 ， 20 斤 没 问题 的...\n",
       "5                            加 V 心 ： snow0548 ， 有 这个 哟\n",
       "6                       充电 打 伙计 28 包邮   喜欢 加 唯心 N07543\n",
       "7                                              欢乐 抓 娃娃\n",
       "8              有没有 玩 欢乐 抓 娃娃 的 呀 ， 输入 我 的 邀请 码 可以 得币 呦\n",
       "9         欢乐 抓 娃娃 ， 邀请 码 963280 得 30 个币 ， 抓 到 娃娃 免费 到家\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chinese_str(s):\n",
    "    out = ''\n",
    "    #mystr = string.decode('utf-8')\n",
    "    mystr = s\n",
    "    mystr = mystr[:100]\n",
    "    for letter in mystr:\n",
    "        out += letter + ' '\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15586"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = int(len(df_train) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14027"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = texts[0:train_count]\n",
    "Y_train = y_labels[0:train_count]\n",
    "X_test = texts[train_count:]\n",
    "Y_test = y_labels[train_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_word_ids = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "x_test_word_ids = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1603,\n",
       " 40,\n",
       " 417,\n",
       " 14389,\n",
       " 530,\n",
       " 330,\n",
       " 950,\n",
       " 931,\n",
       " 110,\n",
       " 4499,\n",
       " 4500,\n",
       " 4501,\n",
       " 14390,\n",
       " 2498,\n",
       " 1604,\n",
       " 5154,\n",
       " 6,\n",
       " 1437,\n",
       " 23]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_word_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded_seqs = keras.preprocessing.sequence.pad_sequences(x_train_word_ids, padding='post', truncating='post', maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_padded_seqs = keras.preprocessing.sequence.pad_sequences(x_test_word_ids,padding='post', truncating='post', maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1603,    40,   417, 14389,   530,   330,   950,   931,   110,\n",
       "        4499,  4500,  4501, 14390,  2498,  1604,  5154,     6,  1437,\n",
       "          23,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0], dtype=int32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train_padded_seqs=np.expand_dims(x_train_padded_seqs,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1603],\n",
       "       [   40],\n",
       "       [  417],\n",
       "       [14389],\n",
       "       [  530],\n",
       "       [  330],\n",
       "       [  950],\n",
       "       [  931],\n",
       "       [  110],\n",
       "       [ 4499],\n",
       "       [ 4500],\n",
       "       [ 4501],\n",
       "       [14390],\n",
       "       [ 2498],\n",
       "       [ 1604],\n",
       "       [ 5154],\n",
       "       [    6],\n",
       "       [ 1437],\n",
       "       [   23],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0],\n",
       "       [    0]], dtype=int32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_padded_seqs=np.expand_dims(x_test_padded_seqs,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.LSTM(256, dropout=0.5, recurrent_dropout=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yuzuo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 14027 samples, validate on 1559 samples\n",
      "Epoch 1/12\n",
      "14027/14027 [==============================] - 106s 8ms/step - loss: 0.6713 - acc: 0.5568 - val_loss: 0.7875 - val_acc: 0.7915\n",
      "Epoch 2/12\n",
      "14027/14027 [==============================] - 101s 7ms/step - loss: 0.6598 - acc: 0.5701 - val_loss: 0.9723 - val_acc: 0.4195\n",
      "Epoch 3/12\n",
      "14027/14027 [==============================] - 101s 7ms/step - loss: 0.6510 - acc: 0.6065 - val_loss: 0.7444 - val_acc: 0.6985\n",
      "Epoch 4/12\n",
      "14027/14027 [==============================] - 106s 8ms/step - loss: 0.6159 - acc: 0.6613 - val_loss: 0.7693 - val_acc: 0.6113\n",
      "Epoch 5/12\n",
      "14027/14027 [==============================] - 101s 7ms/step - loss: 0.6067 - acc: 0.6674 - val_loss: 0.7638 - val_acc: 0.6671\n",
      "Epoch 6/12\n",
      "14027/14027 [==============================] - 103s 7ms/step - loss: 0.6051 - acc: 0.6618 - val_loss: 0.8045 - val_acc: 0.5568\n",
      "Epoch 7/12\n",
      "14027/14027 [==============================] - 103s 7ms/step - loss: 0.5974 - acc: 0.6697 - val_loss: 0.8064 - val_acc: 0.5337\n",
      "Epoch 8/12\n",
      "14027/14027 [==============================] - 106s 8ms/step - loss: 0.5971 - acc: 0.6668 - val_loss: 1.2892 - val_acc: 0.1822\n",
      "Epoch 9/12\n",
      "14027/14027 [==============================] - 106s 8ms/step - loss: 0.5941 - acc: 0.6710 - val_loss: 1.0474 - val_acc: 0.1777\n",
      "Epoch 10/12\n",
      "14027/14027 [==============================] - 110s 8ms/step - loss: 0.5936 - acc: 0.6721 - val_loss: 0.8910 - val_acc: 0.5221\n",
      "Epoch 11/12\n",
      "14027/14027 [==============================] - 103s 7ms/step - loss: 0.5860 - acc: 0.6778 - val_loss: 0.7669 - val_acc: 0.5818\n",
      "Epoch 12/12\n",
      "14027/14027 [==============================] - 95s 7ms/step - loss: 0.5830 - acc: 0.6764 - val_loss: 1.1105 - val_acc: 0.3156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a35a60a90>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_padded_seqs, Y_train, batch_size=32, epochs=12, validation_data=(x_test_padded_seqs, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.add(keras.layers.Input(shape=(100,), dtype='int32'))\n",
    "model2.add(keras.layers.Embedding(len(tokenizer.word_index)+1, 300, input_length=100))\n",
    "model2.add(keras.layers.LSTM(256, dropout=0.5, recurrent_dropout=0.1))\n",
    "model2.add(keras.layers.Dense(256, activation='relu'))\n",
    "model2.add(keras.layers.Dense(2, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(x_train_word_ids, padding='post', truncating='post', maxlen=100)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(x_test_word_ids, padding='post', truncating='post', maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14027 samples, validate on 1559 samples\n",
      "Epoch 1/12\n",
      "14027/14027 [==============================] - 267s 19ms/step - loss: 0.6865 - acc: 0.5492 - val_loss: 0.8714 - val_acc: 0.0000e+00\n",
      "Epoch 2/12\n",
      "14027/14027 [==============================] - 245s 17ms/step - loss: 0.5014 - acc: 0.7168 - val_loss: 0.5684 - val_acc: 0.7389\n",
      "Epoch 3/12\n",
      "14027/14027 [==============================] - 250s 18ms/step - loss: 0.1328 - acc: 0.9591 - val_loss: 0.8022 - val_acc: 0.6863\n",
      "Epoch 4/12\n",
      "14027/14027 [==============================] - 255s 18ms/step - loss: 0.0671 - acc: 0.9795 - val_loss: 1.0065 - val_acc: 0.6985\n",
      "Epoch 5/12\n",
      "14027/14027 [==============================] - 240s 17ms/step - loss: 0.0381 - acc: 0.9894 - val_loss: 0.6628 - val_acc: 0.8127\n",
      "Epoch 6/12\n",
      "14027/14027 [==============================] - 238s 17ms/step - loss: 0.0249 - acc: 0.9934 - val_loss: 0.9199 - val_acc: 0.7415\n",
      "Epoch 7/12\n",
      "14027/14027 [==============================] - 222s 16ms/step - loss: 0.0199 - acc: 0.9949 - val_loss: 1.3440 - val_acc: 0.6453\n",
      "Epoch 8/12\n",
      "14027/14027 [==============================] - 225s 16ms/step - loss: 0.0186 - acc: 0.9948 - val_loss: 0.8575 - val_acc: 0.7460\n",
      "Epoch 9/12\n",
      "14027/14027 [==============================] - 223s 16ms/step - loss: 0.0117 - acc: 0.9971 - val_loss: 1.3132 - val_acc: 0.7024\n",
      "Epoch 10/12\n",
      "14027/14027 [==============================] - 224s 16ms/step - loss: 0.0119 - acc: 0.9966 - val_loss: 1.3928 - val_acc: 0.6697\n",
      "Epoch 11/12\n",
      "14027/14027 [==============================] - 226s 16ms/step - loss: 0.0082 - acc: 0.9977 - val_loss: 1.6068 - val_acc: 0.6908\n",
      "Epoch 12/12\n",
      "14027/14027 [==============================] - 226s 16ms/step - loss: 0.0076 - acc: 0.9983 - val_loss: 1.8530 - val_acc: 0.6491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a34879e80>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, Y_train, batch_size=32, epochs=12, validation_data=(X_test\n",
    "                                                                                 , Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
